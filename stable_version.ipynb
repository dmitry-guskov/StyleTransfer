{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import random\n",
    "from colour.io.luts.iridas_cube import read_LUT_IridasCube\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epochs, model, optimizer, pretrained,name=''):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \n",
    "    Args:\n",
    "    - epochs (int): Number of epochs trained.\n",
    "    - model: The neural network model.\n",
    "    - optimizer: The optimizer used during training.\n",
    "    - pretrained (bool): Indicator if the model was pretrained or not.\n",
    "    \n",
    "    Saves the model state, optimizer state, epoch number, and loss.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, f\"./outputs/model_{name}_pretrained_{pretrained}.pth\")\n",
    "\n",
    "def load_model_from_path(model, optimizer, path):\n",
    "    \"\"\"\n",
    "    Function to load a saved model from a specified file path.\n",
    "\n",
    "    Args:\n",
    "    - model: The neural network model.\n",
    "    - optimizer: The optimizer used during training.\n",
    "    - path (str): The file path to the saved model.\n",
    "\n",
    "    Returns:\n",
    "    - model: The loaded neural network model.\n",
    "    - optimizer: The loaded optimizer state.\n",
    "    - epoch: The epoch number from the loaded model.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    return model, optimizer, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_lut(lut_path):\n",
    "    \"\"\"\n",
    "    Reads a LUT from the specified path, returning instance of LUT3D or LUT3x1D.\n",
    "    \n",
    "    Args:\n",
    "    - lut_path (str): the path to the file from which to read the LUT.\n",
    "    \n",
    "    Returns:\n",
    "    - lut: Instance of LUT3D or LUT3x1D.\n",
    "    \"\"\"\n",
    "    lut = read_LUT_IridasCube(lut_path)\n",
    "    lut.name = os.path.splitext(os.path.basename(lut_path))[0]\n",
    "\n",
    "    return lut\n",
    "\n",
    "\n",
    "def process_image(im, lut):\n",
    "    \"\"\"Applies LUT transformation to the image.\n",
    "    \n",
    "    Args:\n",
    "    - im (torch.Tensor): Input image tensor.\n",
    "    - lut: CubeLUT object containing LUT.\n",
    "    \n",
    "    Returns:\n",
    "    - new_im (torch.Tensor): Transformed image tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # im = im.clamp(0, 1)  # Ensure input tensor values are between 0 and 1\n",
    "    # If the pixel values are outside the range [0, 1], normalize them\n",
    "    scale = False\n",
    "    if im.min() < 0 or im.max() > 1:\n",
    "        delta =  (im.max() - im.min())\n",
    "        imin = im.min()\n",
    "        im = (im - im.min()) / (im.max() - im.min())\n",
    "        scale = True\n",
    "\n",
    "    #image [c,h,w]\n",
    "    # Convert tensors to numpy arrays\n",
    "    im_array = (im.permute(1, 2, 0).cpu().numpy() ).astype(np.float32)\n",
    "    im_array = lut.apply(im_array)  # Apply LUT transformation \n",
    "    \n",
    "    # Convert numpy array back to torch tensor\n",
    "    new_im = torch.from_numpy(im_array).to(im.device).permute(2, 0, 1).float()\n",
    "    \n",
    "    if scale:\n",
    "        new_im = new_im*delta + imin\n",
    "        \n",
    "    return new_im\n",
    "\n",
    "\n",
    "# Define a class for applying random style deformations to images\n",
    "class RandomStyleDeformationWithLUT:\n",
    "    def __init__(self, lut_path):\n",
    "        self.lut_path = lut_path\n",
    "        self.lut_files = self.get_lut_files()\n",
    "\n",
    "    def get_lut_files(self):\n",
    "        return [self.lut_path + '/' + file for file in os.listdir(self.lut_path)  if file.endswith('.cube')]\n",
    "\n",
    "    def apply_lut_transformation(self, images, lut_data):\n",
    "        # Apply LUT transformation to the image\n",
    "        transformed_images = []\n",
    "        for batch in range(images.size(0)):\n",
    "            image = images[batch,:,:,:]\n",
    "            # image [c,h,w]\n",
    "            transformed_image = process_image(image, lut_data)\n",
    "            transformed_images.append(transformed_image)\n",
    "        return torch.stack(transformed_images)\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        '''Returns a function with predownloaded lut f(images) that returns transformed images\n",
    "            Can proccess batch \n",
    "        Args for inner function:\n",
    "            images (torch): _description_\n",
    "\n",
    "        Returns a function that outputs:\n",
    "            torch: _description_\n",
    "        '''\n",
    "        # batch as [b,c,h,w]\n",
    "        # A good question here, whether I should pick just 2 transformations for batch or pick random for each image\n",
    "        selected_lut_file = random.choice(self.lut_files)\n",
    "        lut_data = read_lut(selected_lut_file)\n",
    "        def f(images):\n",
    "            transformed_images = self.apply_lut_transformation(images, lut_data)\n",
    "            return transformed_images\n",
    "        return f\n",
    "\n",
    "\n",
    "class RandomStyleTransformationWithColorJitter:\n",
    "    def __init__(self, brightness=(0.1,2.0), contrast=(0.1,2.0), saturation=(0.1,2.0), hue=(-0.5,0.5)):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "        self.hue = hue\n",
    "\n",
    "    def apply_color_jitter(self, images, params):\n",
    "        \n",
    "        transformed_images = TF.adjust_brightness(images, params[0])\n",
    "        transformed_images = TF.adjust_contrast(transformed_images, params[1])\n",
    "        transformed_images = TF.adjust_saturation(transformed_images, params[2])\n",
    "        transformed_images = TF.adjust_hue(transformed_images, params[3])\n",
    "\n",
    "        return transformed_images\n",
    "\n",
    "    def __call__(self):\n",
    "        params = np.random.uniform([self.brightness[0],self.contrast[0],self.saturation[0],self.hue[0]],[self.brightness[1],self.contrast[1],self.saturation[1],self.hue[1]])\n",
    "        def f(images):\n",
    "            return self.apply_color_jitter(images,params)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a custom dataset for your test images\n",
    "class TestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, filename) for filename in os.listdir(root_dir) if filename.endswith(('.jpeg', '.jpg', '.png'))]\n",
    "        data = []\n",
    "        for img_path in self.image_paths:\n",
    "            image = read_image(img_path)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            data.append(image)\n",
    "        self.data =  torch.FloatTensor(data,device='cuda')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the height and width for resizing\n",
    "height, width = 1536, 1536\n",
    "\n",
    "# Define image transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((height, width)),\n",
    "    # transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Path to your test images folder\n",
    "data_folder = 'data/images'\n",
    "\n",
    "# Create a DataLoader for your test images\n",
    "dataset = TestImageDataset(data_folder, transform=image_transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=24, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the MatrixProductModel\n",
    "class MatrixProductModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MatrixProductModel, self).__init__()\n",
    "        \n",
    "        self.R = nn.Parameter(torch.randn(input_dim, output_dim), requires_grad=True)\n",
    "        self.Q = nn.Parameter(torch.randn(output_dim, input_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, T):\n",
    "        # Ensure x and T have compatible dimensions for batch operations\n",
    "        # Reshape the input batch to batch_size x num_pixels x 3\n",
    "        # Reshape T to batch_size x k x k\n",
    "\n",
    "        # Perform matrix-vector multiplication Q^T * T * R * x\n",
    "        \n",
    "        y =  torch.matmul(torch.matmul(torch.matmul(x, self.R), T), self.Q)\n",
    "        # print(x.size(),self.R.size(),T.size(),self.Q.size())\n",
    "        # print(y.size())\n",
    "        return y.view(y.size(0), -1, y.size(-1))  # Reshape output back to batch format\n",
    "\n",
    "\n",
    "# Define the NeuralPreset model with nn.ModuleList that containes efficientnet_b0, model_n, model_s\n",
    "class NeuralPreset(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=16, image_style_transform=None, image_inner_dim=100):\n",
    "        super(NeuralPreset, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.model_n =  MatrixProductModel(input_dim, output_dim)\n",
    "        self.model_s =  MatrixProductModel(input_dim, output_dim)\n",
    "\n",
    "\n",
    "        self.efficientnet_b0 = torchvision.models.efficientnet_b0()\n",
    "        # in case if you want to download the custom model\n",
    "        # self.efficientnet_b0 = torch.load('efficientnet_b0.pth').eval()\n",
    "        \n",
    "        # Replace the final classifier with a new fully connected layer which ouputs 2*output_dim*output_dim tensor\n",
    "        in_features = self.efficientnet_b0.classifier[-1].in_features\n",
    "        self.efficientnet_b0.classifier[-1] = nn.Linear(in_features, 2 * output_dim * output_dim)  # 2 stands for two vectors, r and d\n",
    "        # Freeze the first layer\n",
    "        for i, layer in enumerate(self.efficientnet_b0.children()):\n",
    "            if i < 2:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        self.efficientNet_transform = models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "        # transforms.Compose([\n",
    "        #                                 transforms.ToPILImage(),\n",
    "        #                                 transforms.Resize(256),\n",
    "        #                                 transforms.CenterCrop(224),\n",
    "        #                                 transforms.ToTensor(),\n",
    "        #                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "        #                                                      std=[0.229, 0.224, 0.225]),]\n",
    "        #                                 )\n",
    "        # models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "\n",
    "        #Style transform for learning pipeline\n",
    "        self.image_style_transform = image_style_transform\n",
    "\n",
    "        # Dimension of image representation to validate the pipeline \n",
    "        self.image_inner_dim = image_inner_dim\n",
    " \n",
    " \n",
    "    def forward(self, Images):\n",
    "        # Get T from the encoder\n",
    "        # images.shape is [batch, 3, width, height]\n",
    "        downsampled_x = self.efficientNet_transform(Images) # [b,c,h,w]\n",
    "        # Shape on this step is [batch, 3(rgb), 224, 224]\n",
    "\n",
    "        # we want to extract the same image style transfer inside batch to apply later on trim_Images\n",
    "        style_transform1 = self.image_style_transform()\n",
    "        style_transform2 = self.image_style_transform()\n",
    "        downsampled_x1 = style_transform1(downsampled_x)\n",
    "        downsampled_x2 = style_transform2(downsampled_x)\n",
    "        \n",
    "        # Output features = 256, so k is fixed to 16\n",
    "        # d_i, r_i - normalized color space, color style\n",
    "        d_1, r_1 = self.efficientnet_b0(downsampled_x1).chunk(2, dim=1)\n",
    "        d_2, r_2 = self.efficientnet_b0(downsampled_x2).chunk(2, dim=1)\n",
    "\n",
    "        k = self.output_dim\n",
    "\n",
    "        # make random indices to proccess not the whole image\n",
    "        rand_ind = np.random.choice(range(downsampled_x.size(2)**2), self.image_inner_dim**2, replace=False)\n",
    "\n",
    "        # Perform operations using model_n and model_s\n",
    "        #[b,c,w,h]\n",
    "        trim_Images = Images.permute([0,2,3,1]).view(Images.size(0),-1,3)[:,rand_ind,:].float()\n",
    "\n",
    "        Z_1 = self.model_n(trim_Images, d_1.view(-1, k, k))  # Modify inputs for batch processing\n",
    "        Z_2 = self.model_n(trim_Images, d_2.view(-1, k, k))  # Modify inputs for batch processing\n",
    "\n",
    "        Y_1 = self.model_s(Z_2, r_1.view(-1, k, k))  # Modify inputs for batch processing\n",
    "        Y_2 = self.model_s(Z_1, r_2.view(-1, k, k))  # Modify inputs for batch processing\n",
    "\n",
    "        i_1 = style_transform1(trim_Images.view(Images.size(0),self.image_inner_dim,self.image_inner_dim,3).permute([0,3,1,2])).permute([0,2,3,1]).view(Images.size(0),-1,3)\n",
    "        i_2 = style_transform2(trim_Images.view(Images.size(0),self.image_inner_dim,self.image_inner_dim,3).permute([0,3,1,2])).permute([0,2,3,1]).view(Images.size(0),-1,3)\n",
    "        #TODO there is no reshape on output, i.e. the optup in the form of vector\n",
    "        # print(Z_1.size(),Y_1.size(),i_1.size())\n",
    "        return Z_1, Z_2, Y_1, Y_2, i_1, i_2\n",
    "    \n",
    "    def modeln(self, image, d):\n",
    "        '''Color-normalizes the image based on the content vector d.\n",
    "\n",
    "        Args:\n",
    "            image (_type_): _description_\n",
    "            d (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        '''        \n",
    "        k = self.output_dim\n",
    "        Z = self.model_n(image.view(image.size(0), -1, 3), d.view(-1, k, k))\n",
    "        return Z\n",
    "    # input image should be in shape (batch_size, width*height, 3)\n",
    "    def models(self, image, r):\n",
    "        '''Stylizes the image based on the style vector r.\n",
    "\n",
    "        Args:\n",
    "            image (_type_): _description_\n",
    "            r (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        '''        \n",
    "        k = self.output_dim\n",
    "        Y = self.model_s(image.view(image.size(0), -1, 3), r.view(-1, k, k))\n",
    "        return Y\n",
    "        # model output in shape (batch_size, width*height, 3)\n",
    "    def encoder(self, image):\n",
    "        '''Downsamples the image and returns the content vector d and style vector r.\n",
    "        Args:\n",
    "            image (_type_): _description_\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        '''        \n",
    "        downsampled = self.efficientNet_transform(image)\n",
    "        d, r = self.efficientnet_b0(downsampled).chunk(2, dim=1)\n",
    "        return d, r\n",
    "    \n",
    "# Custom loss function that combines L1 norms and L2 norm\n",
    "def custom_loss(Z_1, Z_2, Y_1, Y_2, i_1, i_2, coef_l):\n",
    "    \n",
    "    # Calculate L2 norm\n",
    "    l2_norm = F.mse_loss(Z_1, Z_2)\n",
    "\n",
    "    # Calculate L1 norms\n",
    "    l1_norm1 = F.l1_loss(Y_1, i_1)\n",
    "    l1_norm2 = F.l1_loss(Y_2, i_2)\n",
    "\n",
    "    # Combine the L1 norms as needed\n",
    "    loss = coef_l * l2_norm + l1_norm1 + l1_norm2\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the height and width for resizing\n",
    "height, width = 1536, 1536\n",
    "\n",
    "# Define image transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((height, width)),\n",
    "    # transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Path to your test images folder\n",
    "data_folder = 'data/images'\n",
    "\n",
    "# Create a DataLoader for your test images\n",
    "dataset = TestImageDataset(data_folder, transform=image_transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=24, shuffle=False)\n",
    "\n",
    "# models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "# Create a RandomStyleDeformation object\n",
    "random_style_deformation = RandomStyleTransformationWithColorJitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of classes in your task (adjust as needed)\n",
    "k = 16\n",
    "\n",
    "# Initialize the model and optimizer with Adam\n",
    "model = NeuralPreset(3,k,random_style_deformation,224).to(device)\n",
    "# the downsampled size is 224x224x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda coefficient\n",
    "coef_l = 10.0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs =  32# 32\n",
    "log_interval = 10\n",
    "\n",
    "number_of_batches = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955b9ca9999a43c984f4d540b7931dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.train() \n",
    "save = True\n",
    "save_name = 'scale'\n",
    "progress_bar = tqdm(range(num_epochs), total=num_epochs)\n",
    "for epoch in progress_bar:\n",
    "    if epoch==32:\n",
    "        coef_l*=0.1\n",
    "    #TODO eval every\n",
    "    for batch_idx, images in enumerate(dataloader):\n",
    "        if number_of_batches is not None:\n",
    "            if batch_idx == number_of_batches:\n",
    "                break\n",
    "        # images.shape is [batch, 3, width, height]\n",
    "        Z_1, Z_2, Y_1, Y_2, i_1, i_2 = model(images.to(device))\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate loss, coef lambda corresponds to normolize space consistancy or to reconstructed pictures \n",
    "        loss = custom_loss(Z_1, Z_2, Y_1, Y_2, i_1, i_2, coef_l)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Print the loss value at each step\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()}, refresh=False)\n",
    "        if (batch_idx + 1) % log_interval == 0 and save:\n",
    "            save_model(num_epochs,model,optimizer=optimizer,pretrained=True,name=save_name)\n",
    "        #     print(\n",
    "        #         f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "        #         f\"Batch [{batch_idx + 1}/{len(dataloader)}] \"\n",
    "        #         f\"Loss: {loss.item():.6f}\"\n",
    "        #         )\n",
    "if save:        \n",
    "    save_model(num_epochs,model,optimizer=optimizer,pretrained=True,name=save_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
